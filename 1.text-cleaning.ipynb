{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing for Italian Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install bs4 nltk seaborn wordcloud autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import itertools\n",
        "from autocorrect import Speller\n",
        "\n",
        "# Set a clean visual style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")  # or \"talk\" for presentations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load NLTK resources (from the **resources** folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nltk.download('punkt') # Tokenizer models\n",
        "# nltk.download('punkt_tab') # Tokenizer models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This setup is needed because there is a known bug with **italian** resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add resource folder to path\n",
        "base_dir = os.getcwd()  # use working dir\n",
        "resources_path = os.path.join(base_dir, \"resources\")\n",
        "\n",
        "# Load italian tokenize\n",
        "with open(os.path.join(resources_path, \"italian_py3.pickle\"), 'rb') as f:\n",
        "    italian_tokenizer = pickle.load(f)\n",
        "\n",
        "# Load italian stopwords\n",
        "with open(os.path.join(resources_path, \"stopwords_it.txt\"), 'r', encoding='utf-8') as f:\n",
        "    stop_words = set(line.strip() for line in f if line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import dataset from **Hugging Face** (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install datasets\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# Load the CHANGE-IT dataset from Hugging Face\n",
        "# dataset = load_dataset(\"gsarti/change_it\", split=\"train\")\n",
        "\n",
        "# Convert Hugging Face dataset to Pandas DataFrame\n",
        "# df = dataset.to_pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample **change-it** public dataset (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I generate the datasets sampling change-it datasets (you don't need to run this code)\n",
        "\n",
        "# SAMPLE_FRAC = 0.01 # 1% sample\n",
        "\n",
        "# Load datasets\n",
        "# df_repubblica = pd.read_csv(\"change-it/change-it.repubblica.train.csv\", sep=',')\n",
        "# df_ilgiornale = pd.read_csv(\"change-it/change-it.ilgiornale.train.csv\", sep=',')\n",
        "\n",
        "# Estract a 1% sample\n",
        "# df_repubblica_sample = df_repubblica.sample(frac=SAMPLE_FRAC, random_state=42)\n",
        "# df_ilgiornale_sample = df_ilgiornale.sample(frac=SAMPLE_FRAC, random_state=42)\n",
        "\n",
        "# Salva the sample\n",
        "# df_repubblica_sample.to_csv(\"data/repubblica_sample.csv\", index=False)\n",
        "# df_ilgiornale_sample.to_csv(\"data/ilgiornale_sample.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset (stored in **data** folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/repubblica_sample.csv\")\n",
        "\n",
        "# add a column with the newspaper name\n",
        "df['newspaper'] = 'repubblica'\n",
        "\n",
        "# Print df columns\n",
        "print(df.columns)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "\n",
        "def apply_text_cleaning(text):\n",
        "    # 1. Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. Normalize curly quotes and dashes\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\") \\\n",
        "               .replace(\"“\", '\"').replace(\"”\", '\"') \\\n",
        "               .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "\n",
        "    # Optional: normalize Unicode characters to ASCII (e.g., é → e)\n",
        "    # text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
        "\n",
        "    # 3. Remove URLs and email addresses\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # 4. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 5. Remove punctuation (keep straight apostrophes and dashes)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "\n",
        "    # 6. Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 7. Tokenize and remove stopwords\n",
        "    sentences = italian_tokenizer.tokenize(text)\n",
        "    tokens = [word for sent in sentences for word in sent.split()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create italian stemmer\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "\n",
        "def apply_stemming(text):\n",
        "    tokens = text.split()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Spell checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the spell checker once for efficiency\n",
        "spell = Speller(lang='it')\n",
        "\n",
        "# Define vowels including accented ones\n",
        "vowels = set('aeiouAEIOUàèéìòùÀÈÉÌÒÙ')\n",
        "\n",
        "def apply_spell_check(text):\n",
        "    \"\"\"\n",
        "    Reduces repeated vowels and consonants in a word \n",
        "    (1 max for vowels including accented, 2 max for consonants),\n",
        "    then applies spell correction.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to correct\n",
        "\n",
        "    Returns:\n",
        "        str: Corrected text\n",
        "    \"\"\"\n",
        "    # Group consecutive identical characters\n",
        "    grouped_chars = itertools.groupby(text)\n",
        "\n",
        "    # Apply max 1 repetition for vowels, max 2 for consonants\n",
        "    cleaned_text = ''.join(\n",
        "        ''.join(group)[:1] if char in vowels else ''.join(group)[:2]\n",
        "        for char, group in grouped_chars\n",
        "    )\n",
        "\n",
        "    # Apply spell correction\n",
        "    corrected_text = spell(cleaned_text)\n",
        "    \n",
        "    return corrected_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try methods on a sample sentence (playground)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_text = \"I giornalisti stavano scrivendo articoli mooooooltttttto interessanti sull'inteligensa artificiale.\"\n",
        "print(\"Sample text:\", sample_text)\n",
        "\n",
        "# Phase 1: spell checking\n",
        "corrected = apply_spell_check(sample_text)\n",
        "print(\"After spell checking:\", corrected)\n",
        "\n",
        "# Phase 2: cleaning\n",
        "cleaned = apply_text_cleaning(corrected)\n",
        "print(\"After cleaning text:\", cleaned)\n",
        "\n",
        "# Phase 3: stemming\n",
        "stemmed = apply_stemming(cleaned)\n",
        "print(\"After stemming:\", stemmed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply text cleaning to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply text cleaning\n",
        "df['cleaned_text'] = df['full_text'].apply(apply_text_cleaning)\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute word frequency (using **Counter** method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join all cleaned texts into a single list of words\n",
        "all_words = ' '.join(df['cleaned_text']).split()\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the 10 most frequent words\n",
        "print(\"Most frequent words:\")\n",
        "print(word_freq.most_common(10))\n",
        "\n",
        "# Show the 10 least frequent words\n",
        "print(\"\\nLeast frequent words:\")\n",
        "print(word_freq.most_common()[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🛠️ Define a function to plot word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_top_words(word_freq, title, top_n=10, color='#4C72B0'):\n",
        "    # Get the top N words and their counts\n",
        "    words, counts = zip(*word_freq.most_common(top_n))\n",
        "    \n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = sns.barplot(x=list(words), y=list(counts), color=color)\n",
        "\n",
        "    # Annotate bars with counts\n",
        "    for i, count in enumerate(counts):\n",
        "        bars.text(i, count + max(counts)*0.01, str(count), \n",
        "                  ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Improve aesthetics\n",
        "    plt.title(title, fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(\"Words\", fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.xticks(rotation=45, fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply the plot_top_word to before/after word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Recalculate frequencies\n",
        "freq_before = Counter(' '.join(df['full_text']).split())\n",
        "freq_after = Counter(' '.join(df['cleaned_text']).split())\n",
        "\n",
        "# Plot comparison\n",
        "plot_top_words(freq_before, \"Top 10 Most Frequent Words (Before Filtering)\", color='#1f77b4')\n",
        "plot_top_words(freq_after, \"Top 10 Most Frequent Words (After Filtering)\", color='#ff7f0e')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define frequent and rare word sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sets of most and least frequent words\n",
        "most_common = set([word for word in word_freq.most_common(10)])\n",
        "least_common = set([word for word in word_freq.most_common()[-10:]])\n",
        "\n",
        "print(\"Words to remove (most frequent):\", most_common)\n",
        "print(\"Words to remove (least frequent):\", least_common)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🛠️ Define the filtering function  (remove **least_common** and **unwanted** words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove both most and least frequent words from a text\n",
        "def remove_rare_words(text):\n",
        "    tokens = text.split()\n",
        "    return ' '.join([word for word in tokens if word not in least_common])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove a list of unwanted words from a text\n",
        "def remove_custom_words(text, words_to_remove):\n",
        "    tokens = text.split()\n",
        "    cleaned_tokens = [word for word in tokens if word not in words_to_remove]\n",
        "    return ' '.join(cleaned_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🧪 Apply filtering and compare results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words_to_remove = ['cè', 'litalia', 'alcune', 'né']\n",
        "\n",
        "# Apply the filtering function to the cleaned texts\n",
        "df['cleaned_text_no_rare'] = df['cleaned_text'].apply(remove_rare_words)\n",
        "\n",
        "# Apply the filtering function to remove the unwanted words\n",
        "df['final_text'] = df['cleaned_text_no_rare'].apply(lambda x: remove_custom_words(x, words_to_remove))\n",
        "\n",
        "# Show comparison between original, cleaned, and final versions\n",
        "df[['full_text', 'cleaned_text', 'final_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ☁️ Generate word clouds (before and after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word cloud from cleaned_text (before filtering)\n",
        "text_before = ' '.join(df['full_text'])\n",
        "wordcloud_before = WordCloud(width=800, height=400, background_color='white').generate(text_before)\n",
        "\n",
        "# Create word cloud from final_text (after filtering)\n",
        "text_after = ' '.join(df['final_text'])\n",
        "wordcloud_after = WordCloud(width=800, height=400, background_color='white').generate(text_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the figure\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Word cloud before filtering\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud_before, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud – Before Filtering\", fontsize=14)\n",
        "plt.axis('off')\n",
        "\n",
        "# Word cloud after filtering\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud_after, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud – After Filtering\", fontsize=14)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save cleaned dataset in **data** folder (_repubblica_cleaned.csv_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned = df[['headline','full_text','final_text']]\n",
        "df_cleaned.to_csv(\"data/repubblica_cleaned.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
